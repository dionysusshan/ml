{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZC/2Cjs8dtoclWJ6LeoIu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dionysusshan/ml/blob/main/Blending_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToxOtff-11jY",
        "outputId": "fa5e60fc-92b1-4bff-cb69-5b155d85d573"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "    AQI-IN    PM25    PM10     PM1  Temp(cel)     Hum   Noise  TVOC(ppm)  \\\n",
            "0  114.358  64.170  85.939  60.400     22.607  98.277  48.345      0.009   \n",
            "1   95.474  57.260  75.377  53.974     22.373  95.771  48.370      0.010   \n",
            "2   78.380  47.045  59.341  44.598     24.292  85.416  48.078      0.008   \n",
            "3   65.078  39.044  47.500  37.322     28.122  71.982  50.844      0.008   \n",
            "4   59.369  35.598  42.358  34.196     30.972  63.498  50.855      0.010   \n",
            "\n",
            "   CO(ppm)  CO2(ppm)  SO2(ppm)  NO2(ppm)  O3(ppm)  AQI-IN(F)     CI    VI  \\\n",
            "0    0.392   482.552     0.002     0.008    0.021    114.358  9.873  10.0   \n",
            "1    0.454   486.747     0.002     0.008    0.023     95.474  9.006  10.0   \n",
            "2    0.667   482.067     0.003     0.009    0.025     82.128  9.000  10.0   \n",
            "3    0.680   462.433     0.002     0.009    0.026     77.250  9.000  10.0   \n",
            "4    0.697   455.927     0.002     0.009    0.024     76.173  9.000  10.0   \n",
            "\n",
            "   particle count(0 3)  particle count(0 5)  particle count(1 0)  \\\n",
            "0            19366.709             2500.933              365.055   \n",
            "1            17307.818             2267.409              320.416   \n",
            "2            14244.000             1777.883              238.547   \n",
            "3            11881.167             1343.428              170.917   \n",
            "4            10850.045             1147.922              141.916   \n",
            "\n",
            "   particle count(3 0)  \n",
            "0              152.188  \n",
            "1              132.013  \n",
            "2               93.771  \n",
            "3               64.283  \n",
            "4               51.911  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.989010989010989\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      1307\n",
            "           1       1.00      0.88      0.93        49\n",
            "           2       0.00      0.00      0.00         9\n",
            "\n",
            "    accuracy                           0.99      1365\n",
            "   macro avg       0.66      0.63      0.64      1365\n",
            "weighted avg       0.98      0.99      0.99      1365\n",
            "\n",
            "Models and blend predictions saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Load Data\n",
        "file_path = '/content/sample_data/imputed_datasetknn.csv'  # Replace with your actual file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"Original Data:\")\n",
        "print(df.head())\n",
        "\n",
        "# Step 2: Preprocess Data\n",
        "# Assume the last column is the target variable\n",
        "X = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "\n",
        "# Check if the target variable y needs to be converted to categorical\n",
        "if not np.issubdtype(y.dtype, np.integer):\n",
        "    y = pd.cut(y, bins=3, labels=False)  # Example: Convert continuous to 3 categories\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Further split the training set for blending\n",
        "X_train_base, X_blend, y_train_base, y_blend = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features (optional but recommended for consistency across models)\n",
        "scaler = StandardScaler()\n",
        "X_train_base = scaler.fit_transform(X_train_base)\n",
        "X_blend = scaler.transform(X_blend)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 3: Define the Base Models\n",
        "base_models = [\n",
        "    LogisticRegression(random_state=42),\n",
        "    KNeighborsClassifier(n_neighbors=5),\n",
        "    SVC(kernel='rbf', probability=True, random_state=42)\n",
        "]\n",
        "\n",
        "# Step 4: Train the Base Models\n",
        "for model in base_models:\n",
        "    model.fit(X_train_base, y_train_base)\n",
        "\n",
        "# Step 5: Make Predictions on the Blend Set\n",
        "blend_train = np.zeros((X_blend.shape[0], len(base_models)))\n",
        "for i, model in enumerate(base_models):\n",
        "    blend_train[:, i] = model.predict_proba(X_blend)[:, 1]\n",
        "\n",
        "# Step 6: Define the Meta-Model\n",
        "meta_model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Step 7: Train the Meta-Model\n",
        "meta_model.fit(blend_train, y_blend)\n",
        "\n",
        "# Step 8: Make Predictions on the Test Set\n",
        "blend_test = np.zeros((X_test.shape[0], len(base_models)))\n",
        "for i, model in enumerate(base_models):\n",
        "    blend_test[:, i] = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "y_pred = meta_model.predict(blend_test)\n",
        "\n",
        "# Step 9: Evaluate the Predictions\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Classification Report:\\n{report}\")\n",
        "\n",
        "# Optional: Save the models and the blend predictions\n",
        "import joblib\n",
        "for i, model in enumerate(base_models):\n",
        "    joblib.dump(model, f'base_model_{i}.pkl')\n",
        "joblib.dump(meta_model, 'meta_model.pkl')\n",
        "print(\"Models and blend predictions saved.\")\n"
      ]
    }
  ]
}